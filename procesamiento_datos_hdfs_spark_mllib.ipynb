{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15bfcfc880f2897e6836933d55c7c042",
     "grade": false,
     "grade_id": "cell-570cf80ae1b2c48e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# **Procesamiento Masivo de Datos con HDFS, Spark SQL y MLlib**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook debe quedar alojado en la carpeta GCS del cluster creado.\n",
    "\n",
    "Selecciona el kernel de PySpark para ejecutarlo correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10cdca7b33ec4328e542f94942371393",
     "grade": false,
     "grade_id": "cell-42368b0202b6ce77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Análisis y tratamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5310bc8c3244ee98391957d26dc91d08",
     "grade": false,
     "grade_id": "lectura-fichero",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# El archivo flights.csv contiene información sobre vuelos, como el año, el mes, el día, la hora, el origen, el destino, la duración del vuelo, el retraso en la salida y el retraso en la llegada\n",
    "ruta_hdfs = \"/DataCluster_Test/flights.csv\" # Ruta HDFS en la que se encuentra el archivo flights.csv\n",
    "\n",
    "# Cargo el archivo flights.csv en un DataFrame con PySpark\n",
    "# Indico que el archivo contiene encabezados y que intente inferir el esquema.\n",
    "flightsDF = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(ruta_hdfs) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimo el esquema para comprobar si ha inferido correctamente el tipo de dato en cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: string (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.printSchema() # Muestro el esquema\n",
    "\n",
    "flightsDF.count() # Cuento la cantidad de registros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veo que tenemos 162049 registros, coincide con el archivo. Si imprimo por pantalla las 5 primeras filas, veré qué tipos parecen tener y en qué columnas no coincide el tipo que podríamos esperar con el tipo que ha inferido Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|    1|  1|       1|       96|     235|       70|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n",
      "|2014|    1|  1|       4|       -6|     738|      -23|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n",
      "|2014|    1|  1|       8|       13|     548|       -4|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n",
      "|2014|    1|  1|      28|       -2|     800|      -23|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|\n",
      "|2014|    1|  1|      34|       44|     325|       43|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al comparar lo inferido por Spark con el valor que tienen en el dataset algunas columnas, se puede ver que no les ha asignado el tipo de dato correctamente. La causa del problema es que en muchas columnas existe un valor faltante llamado \"NA\". Spark no reconoce ese valor como *no disponible* ni nada similar, sino que lo considera como un string de texto normal, y por tanto, asigna a toda la columna el tipo de dato string. \n",
    "\n",
    "Concretamente, las siguientes columnas deberían ser de tipo entero pero Spark las muestra como string:\n",
    "<ul>\n",
    " <li>dep_time: string (nullable = true)\n",
    " <li>dep_delay: string (nullable = true)\n",
    " <li>arr_time: string (nullable = true)\n",
    " <li>arr_delay: string (nullable = true)\n",
    " <li>air_time: string (nullable = true)\n",
    " <li>hour: string (nullable = true)\n",
    " <li>minute: string (nullable = true)    \n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a averiguar cuántas filas tienen el valor \"NA\" (como string) en la columna dep_time por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "857"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "cuantos_NA = flightsDF\\\n",
    "                .where(F.col(\"dep_time\") == \"NA\")\\\n",
    "                .count()\n",
    "cuantos_NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen 857 filas que no tienen un dato válido en esa columna. Hay distintas maneras de trabajar con los valores faltantes, como por ejemplo imputarlos (reemplazarlos por un valor generado por nosotros según cierta lógica, por ejemplo la media de esa columna, etc). \n",
    "\n",
    "Lo más sencillo es quitar toda la fila, aunque esto depende de si nos lo podemos permitir en base a la cantidad de datos que tenemos. En este caso, el dataset dispone de un número considerable de filas en total en comparación con las que tienen valores nulos, así que quitaré todas las filas donde hay un NA en cualquiera de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/09 10:51:23 WARN org.apache.spark.sql.execution.CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, month: int, day: int, dep_time: string, dep_delay: string, arr_time: string, arr_delay: string, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: string, distance: int, hour: string, minute: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnas_limpiar = [\"dep_time\", \"dep_delay\", \"arr_time\", \"arr_delay\", \"air_time\", \"hour\", \"minute\"]\n",
    "\n",
    "flightsLimpiado = flightsDF\n",
    "for nombreColumna in columnas_limpiar:  # Para cada columna, me quedo solo con las filas que no tienen NA en esa columna\n",
    "    flightsLimpiado = flightsLimpiado.where(F.col(nombreColumna) != \"NA\")\n",
    "\n",
    "flightsLimpiado.cache() # Guardo el DataFrame en memoria para que las operaciones sean más rápidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que se han eliminado los NA, voy a convertir a entero cada una las columnas que erróneamente eran de tipo string.\n",
    "\n",
    "Ahora no debe haber problema ya que todas las cadenas de texto contienen dentro un número que puede ser convertido de texto a número. \n",
    "Convertiré también la columna `arr_delay` de entero a número real, será necesario para los pasos posteriores donde ajustaré un modelo predictivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, month: int, day: int, dep_time: int, dep_delay: int, arr_time: int, arr_delay: double, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: int, distance: int, hour: int, minute: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "flightsConvertido = flightsLimpiado\n",
    "\n",
    "for c in columnas_limpiar: # Método que crea una columna o reemplaza una existente con el mismo nombre, pero con un tipo de dato diferente\n",
    "    flightsConvertido = flightsConvertido.withColumn(c, F.col(c).cast(IntegerType())) \n",
    "\n",
    "flightsConvertido = flightsConvertido.withColumn(\"arr_delay\", F.col(\"arr_delay\").cast(DoubleType())) # Cambio el tipo de dato de la columna arr_delay a DoubleType\n",
    "flightsConvertido.cache()  # Cacheo el DataFrame para que las operaciones sean más rápidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: integer (nullable = true)\n",
      " |-- dep_delay: integer (nullable = true)\n",
      " |-- arr_time: integer (nullable = true)\n",
      " |-- arr_delay: double (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsConvertido.printSchema() # Muestro el esquema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora el esquema si muestra el tipo de datos correcto en cada columna y Spark si está tratando como enteros las columnas que deberían serlo, y si quisiera podría hacer operaciones aritméticas\n",
    "con ellas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5810b869bd7baccabe0a2952dd0baae1",
     "grade": false,
     "grade_id": "cell-c0cfdd1db1edaa7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Tareas varias\n",
    "\n",
    "Partiendo del DataFrame `flightsConvertido` en cada tarea: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tarea 1 -> Crear un nuevo DataFrame llamado `aeropuertosOrigenDF` que tenga una columna `origin` y que tenga tantas filas como aeropuertos distintos de *origen* existan. ¿Cuántas filas tiene? Almacenar dicho recuento en la variable entera `n_origen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8fb2ec5d49ff84edae4833eca797068b",
     "grade": false,
     "grade_id": "ejercicio-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "aeropuertosOrigenDF = flightsConvertido.select(\"origin\").distinct()\n",
    "n_origen = aeropuertosOrigenDF.count()\n",
    "print(n_origen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tarea 2 -> Crear un nuevo DataFrame llamado `rutasDistintasDF` que tenga dos columnas `origin`, `dest` y que tenga tantas filas como rutas diferentes existan (es decir, como combinaciones distintas haya entre un origen y un destino). Una vez creado, contar cuántas combinaciones hay, almacenando dicho recuento en la variable entera `n_rutas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rutasDistintasDF = flightsConvertido.select(\"origin\", \"dest\").distinct()\n",
    "n_rutas = rutasDistintasDF.count()\n",
    "print(n_rutas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tarea 3 -> Calcular, *sólo para los vuelos que llegan con* ***retraso positivo***, el retraso medio a la llegada de dichos vuelos, para cada aeropuerto de destino. La nueva columna con el retraso medio a la llegada debe llamarse `retraso_medio`. El DF resultante debe estar ordenado de mayor a menor retraso medio. El código que calcule esto debería ir encapsulado en una función de Python llamada `retrasoMedio` que reciba como argumento un DataFrame y devuelva como resultado el DataFrame con el cálculo descrito anteriormente.\n",
    "  \n",
    "  Una vez hecha la función, invocarla pasándole como argumento `flightsConvertido` y almacenar el resultado devuelto en la variable `retrasoMedioDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "097436083d0007a7d99d5108e1a504c9",
     "grade": false,
     "grade_id": "ejercicio-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "def retrasoMedio(df):                          # Función que recibe un DataFrame y devuelve un nuevo DataFrame con el retraso medio por destino\n",
    "    df_filtrado = df.filter(df.arr_delay > 0)\n",
    "    df_resultado = df_filtrado.groupBy(\"dest\") \\\n",
    "                              .agg(avg(\"arr_delay\").alias(\"retraso_medio\")) \\\n",
    "                              .orderBy(\"retraso_medio\", ascending=False)\n",
    "    return df_resultado\n",
    "\n",
    "retrasoMedioDF = retrasoMedio(flightsConvertido) # DataFrame con el retraso medio por destino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora llamo a la función `retrasoMedio` pasándole como argumento `flightsConvertido`. \n",
    "- ¿Cuáles son los tres aeropuertos con mayor retraso medio? \n",
    "- ¿Cuáles son sus retrasos medios en minutos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|dest|     retraso_medio|\n",
      "+----+------------------+\n",
      "| BOI|             64.75|\n",
      "| HDN|              46.8|\n",
      "| SFO|41.193768844221104|\n",
      "+----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrasoMedioDF = retrasoMedio(flightsConvertido)\n",
    "retrasoMedioDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa9ed55cd86eb0958e150d6a918db1af",
     "grade": false,
     "grade_id": "cell-e577747d4427e32b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aplicación de Spark MLlib\n",
    "\n",
    "A continuación voy a ajustar un modelo de DecisionTree de Spark para predecir si un vuelo vendrá o no con retraso (problema de clasificación binaria), utilizando como variables predictoras el mes, el día del mes, la hora de partida (`dep_time`), la hora de llegada (`arr_time`), el tipo de avión (`carrier`), la distancia y el tiempo que permanece en el aire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4405b7db7180445df5cacc66d82db53",
     "grade": false,
     "grade_id": "cell-e577747d4427e32a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "En estos datos hay variables numéricas y variables categóricas que ahora mismo están tipadas como numéricas, como por ejemplo el mes del año (`month`), que es en realidad categórica. Hay que indicar a Spark cuáles son categóricas e indexarlas. Para ello tengo que: \n",
    "\n",
    "- Crear un `StringIndexer` al que llamaré `indexerMonth` y otro al que llamaré `indexerCarrier` sobre las variables categóricas `month` y `carrier`. El nombre de las columnas indexadas que se van a crear son, respectivamente, `monthIndexed` y `carrierIndexed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb039584cd14e6bc3434e5be930341e6",
     "grade": false,
     "grade_id": "string-indexer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer # Importo la clase StringIndexer\n",
    "\n",
    "indexerMonth = StringIndexer(inputCol=\"month\", outputCol=\"monthIndexed\") # Creo un objeto StringIndexer para la columna month\n",
    "\n",
    "indexerCarrier = StringIndexer(inputCol=\"carrier\", outputCol=\"carrierIndexed\") # Creo un objeto StringIndexer para la columna carrier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69e024b0baeeb36bb74d136c7e113372",
     "grade": false,
     "grade_id": "cell-e577747d4427e323",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Spark requiere que todas las variables estén en una única columna de tipo vector, por lo que después de indexar estas dos variables, hay que fusionarlas en una columna de tipo vector todas ellas, utilizando un `VectorAssembler`. Por tanto:\n",
    "\n",
    "- Crearé en una variable llamada `vectorAssembler` un `VectorAssembler` que reciba como entrada una lista de todas las variables de entrada (sin incluir `arr_delay`) que serán las que formarán parte del modelo. \n",
    "  \n",
    "    Crearé primero esta lista de variables (lista de strings) en la variable `columnas_ensamblar` y pasaré dicha variable como argumento al crear el `VectorAssembler`. En el caso de las columnas `month` y `carrier`, no usaré las variables originales sino las indexadas en el apartado anterior. La columna de tipo vector creada con las características ensambladas se llamará `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44623471d64d0605fcbcd4bbcc3c2a0d",
     "grade": false,
     "grade_id": "vector-assembler",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler # Importo la clase VectorAssembler\n",
    "\n",
    "columnas_ensamblar = [\"monthIndexed\", \"carrierIndexed\", \"day\", \"dep_time\", \"arr_time\", \"distance\", \"air_time\"] # Columnas que quiero ensamblar\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=columnas_ensamblar, outputCol=\"features\")  # Creo un objeto VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d17b2fa1d8a4bd02b89952429ba1552",
     "grade": true,
     "grade_id": "vector-assembler-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(isinstance(vectorAssembler, VectorAssembler))\n",
    "assert(vectorAssembler.getOutputCol() == \"features\")\n",
    "input_cols = vectorAssembler.getInputCols() \n",
    "assert(len(input_cols) == 7)\n",
    "assert(\"arr_delay\" not in input_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarización de la columna objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8eeb3a3f0b15e8b374789706cd9bce49",
     "grade": false,
     "grade_id": "cell-e577747d4427e32dsdf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finalmente, veo que la columna `arr_delay` es continua, y no binaria como requiere un problema de clasificación con dos clases. Por tanto, voy a convertirla en binaria. Para ello:\n",
    "\n",
    "-  Utilizaré un binarizador de Spark, fijando a 15 el umbral, y guardarlo en la variable `delayBinarizer`. Consideraré retrasado un vuelo que llegue con más de 15 minutos de retraso, y no retrasado en caso contrario. La nueva columna creada con la variable binaria se llamará `arr_delay_binary` y será interpretada como la columna target para el algoritmo. Por ese motivo, esta columna **no** se incluyó en el apartado anterior entre las columnas que se ensamblan para formar las features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f33c493a3c3dd83fb6a39a7acefca5c",
     "grade": false,
     "grade_id": "binarizer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "delayBinarizer = Binarizer(threshold=15, inputCol=\"arr_delay\", outputCol=\"arr_delay_binary\") # Creo un objeto Binarizer para la columna arr_delay con un threshold de 15 minutos de retraso en la llegada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación del modelo de árbol de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5aed16d7ed0219fe1d8741848f594319",
     "grade": false,
     "grade_id": "cell-25a7793978ee7d05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Por último, creo el modelo de clasificación.\n",
    "\n",
    "- Creo en una variable `decisionTree` un árbol de clasificación de Spark (`DecisionTreeClassifier` del paquete `pyspark.ml.classification`)\n",
    "- Indico como columna de entrada la columna creada por el `VectorAssembler`.\n",
    "- Indico como columna objetivo (target) la columna creada por el `Binarizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e785136d6d06691c003ff9542027e03d",
     "grade": false,
     "grade_id": "decision-tree",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "decisionTree = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"arr_delay_binary\") # Creo un objeto DecisionTreeClassifier con las columnas features y arr_delay_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "728da91edabbac5b62baf31bdd0a707e",
     "grade": false,
     "grade_id": "cell-e577747d4427e32d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora encapsulo todas las fases en un sólo pipeline y lo entrenaré:\n",
    "\n",
    "- Crearé en una variable llamada `pipeline` un objeto `Pipeline` de Spark con las etapas anteriores en el orden adecuado para poder entrenar un modelo. \n",
    "\n",
    "- Entrenarlo llamando al método `fit` y guardaré el pipeline entrenado devuelto en una variable llamada `pipelineModel`. \n",
    "\n",
    "- Aplico el pipeline entrenado para predecir el DataFrame `flightsConvertido`, guardando las predicciones devueltas en la variable `flightsPredictions` que será un DataFrame. \n",
    "\n",
    "- Crearé un evaluador con el fin de calcular la precisión del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edbdb627305d03efa41a88426330e160",
     "grade": false,
     "grade_id": "pipeline",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "trainData, testData = flightsConvertido.randomSplit([0.8, 0.2], seed=7) # Divido los datos en entrenamiento y test (80% - 20%)\n",
    "\n",
    "pipeline = Pipeline(stages=[indexerMonth, indexerCarrier, vectorAssembler, delayBinarizer, decisionTree]) # Creo un objeto Pipeline con los distintos objetos\n",
    "\n",
    "pipelineModel = pipeline.fit(flightsConvertido) # Entreno el modelo\n",
    "\n",
    "flightsPredictions = pipelineModel.transform(flightsConvertido) # Realizo las predicciones\n",
    "\n",
    "flightsPredictions.select(\"features\", \"prediction\", \"arr_delay_binary\").show() # Muestro algunas predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c531953abf18cfb3b67571ddde7a57d",
     "grade": false,
     "grade_id": "cell-61156fe5938763f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Muestro la matriz de confusión. Agrupo por la variable que tiene la clase verdadera y la que tiene la clase predicha, para ver en cuántos casos coinciden y en cuántos difieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75f98ae39b827e75c3f0b4b2aaa6b0db",
     "grade": false,
     "grade_id": "cell-896752beb71cb455",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+------+\n",
      "|arr_delay_binary|prediction| count|\n",
      "+----------------+----------+------+\n",
      "|             1.0|       1.0|   530|\n",
      "|             0.0|       1.0|    70|\n",
      "|             1.0|       0.0| 23719|\n",
      "|             0.0|       0.0|136429|\n",
      "+----------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsPredictions.groupBy(\"arr_delay_binary\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator( # Creo un evaluador para la precisión del modelo\n",
    "    labelCol=\"arr_delay_binary\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(flightsPredictions) # Calculo la precisión del modelo\n",
    "print(f\"Accuracy del modelo: {accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
