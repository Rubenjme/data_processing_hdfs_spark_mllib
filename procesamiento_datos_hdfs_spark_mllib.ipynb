{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15bfcfc880f2897e6836933d55c7c042",
     "grade": false,
     "grade_id": "cell-570cf80ae1b2c48e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# **Big Data Pipeline: HDFS, Spark, and Kafka Integration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook debe quedar alojado en la carpeta GCS del cluster creado. Hay que seleccionar el kernel de PySpark para ejecutarlo correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10cdca7b33ec4328e542f94942371393",
     "grade": false,
     "grade_id": "cell-42368b0202b6ce77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Análisis y tratamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5310bc8c3244ee98391957d26dc91d08",
     "grade": false,
     "grade_id": "lectura-fichero",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# El archivo flights.csv contiene información sobre vuelos, como el año, el mes, el día, la hora, el origen, el destino, la duración del vuelo, el retraso en la salida y el retraso en la llegada\n",
    "ruta_hdfs = \"/DataCluster_Test/flights.csv\" # Ruta HDFS en la que se encuentra el archivo flights.csv\n",
    "\n",
    "# Cargo el archivo flights.csv en un DataFrame con PySpark\n",
    "# Indico que el archivo contiene encabezados y que intente inferir el esquema.\n",
    "flightsDF = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(ruta_hdfs) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimo el esquema para comprobar si ha inferido correctamente el tipo de dato en cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: string (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "162049"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightsDF.printSchema() # Muestro el esquema\n",
    "\n",
    "flightsDF.count() # Cuento la cantidad de registros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veo que tenemos 162049 registros, coincide con el archivo. Si imprimo por pantalla las 5 primeras filas, veré qué tipos parecen tener y en qué columnas no coincide el tipo que podríamos esperar con el tipo que ha inferido Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|    1|  1|       1|       96|     235|       70|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n",
      "|2014|    1|  1|       4|       -6|     738|      -23|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n",
      "|2014|    1|  1|       8|       13|     548|       -4|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n",
      "|2014|    1|  1|      28|       -2|     800|      -23|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|\n",
      "|2014|    1|  1|      34|       44|     325|       43|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al comparar lo inferido por Spark con el valor que tienen en el dataset algunas columnas, se puede ver que no les ha asignado el tipo de dato correctamente. La causa del problema es que en muchas columnas existe un valor faltante llamado \"NA\". Spark no reconoce ese valor como *no disponible* ni nada similar, sino que lo considera como un string de texto normal, y por tanto, asigna a toda la columna el tipo de dato string. \n",
    "\n",
    "Concretamente, las siguientes columnas deberían ser de tipo entero pero Spark las muestra como string:\n",
    "<ul>\n",
    " <li>dep_time: string (nullable = true)\n",
    " <li>dep_delay: string (nullable = true)\n",
    " <li>arr_time: string (nullable = true)\n",
    " <li>arr_delay: string (nullable = true)\n",
    " <li>air_time: string (nullable = true)\n",
    " <li>hour: string (nullable = true)\n",
    " <li>minute: string (nullable = true)    \n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a averiguar cuántas filas tienen el valor \"NA\" (como string) en la columna dep_time por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "857"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "cuantos_NA = flightsDF\\\n",
    "                .where(F.col(\"dep_time\") == \"NA\")\\\n",
    "                .count()\n",
    "cuantos_NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen 857 filas que no tienen un dato válido en esa columna. Hay distintas maneras de trabajar con los valores faltantes, como por ejemplo imputarlos (reemplazarlos por un valor generado por nosotros según cierta lógica, por ejemplo la media de esa columna, etc). \n",
    "\n",
    "Lo más sencillo es quitar toda la fila, aunque esto depende de si nos lo podemos permitir en base a la cantidad de datos que tenemos. En este caso, el dataset dispone de un número considerable de filas en total en comparación con las que tienen valores nulos, así que quitaré todas las filas donde hay un NA en cualquiera de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, month: int, day: int, dep_time: string, dep_delay: string, arr_time: string, arr_delay: string, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: string, distance: int, hour: string, minute: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnas_limpiar = [\"dep_time\", \"dep_delay\", \"arr_time\", \"arr_delay\", \"air_time\", \"hour\", \"minute\"]\n",
    "\n",
    "flightsLimpiado = flightsDF\n",
    "for nombreColumna in columnas_limpiar:  # Para cada columna, me quedo solo con las filas que no tienen NA en esa columna\n",
    "    flightsLimpiado = flightsLimpiado.where(F.col(nombreColumna) != \"NA\")\n",
    "\n",
    "flightsLimpiado.cache() # Guardo el DataFrame en memoria para que las operaciones sean más rápidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que se han eliminado los NA, voy a convertir a entero cada una las columnas que erróneamente eran de tipo string.\n",
    "\n",
    "Ahora no debe haber problema ya que todas las cadenas de texto contienen dentro un número que puede ser convertido de texto a número. \n",
    "Convertiré también la columna `arr_delay` de entero a número real, será necesario para los pasos posteriores donde ajustaré un modelo predictivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, month: int, day: int, dep_time: int, dep_delay: int, arr_time: int, arr_delay: double, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: int, distance: int, hour: int, minute: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "flightsConvertido = flightsLimpiado\n",
    "\n",
    "for c in columnas_limpiar: # Método que crea una columna o reemplaza una existente con el mismo nombre, pero con un tipo de dato diferente\n",
    "    flightsConvertido = flightsConvertido.withColumn(c, F.col(c).cast(IntegerType())) \n",
    "\n",
    "flightsConvertido = flightsConvertido.withColumn(\"arr_delay\", F.col(\"arr_delay\").cast(DoubleType())) # Cambio el tipo de dato de la columna arr_delay a DoubleType\n",
    "flightsConvertido.cache()  # Cacheo el DataFrame para que las operaciones sean más rápidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: integer (nullable = true)\n",
      " |-- dep_delay: integer (nullable = true)\n",
      " |-- arr_time: integer (nullable = true)\n",
      " |-- arr_delay: double (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsConvertido.printSchema() # Muestro el esquema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora el esquema si muestra el tipo de datos correcto en cada columna y Spark si está tratando como enteros las columnas que deberían serlo, y si quisiera podría hacer operaciones aritméticas\n",
    "con ellas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5810b869bd7baccabe0a2952dd0baae1",
     "grade": false,
     "grade_id": "cell-c0cfdd1db1edaa7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Tareas varias\n",
    "\n",
    "Partiendo del DataFrame `flightsConvertido` en cada tarea: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Crear un nuevo DataFrame llamado `aeropuertosOrigenDF` que tenga una columna `origin` y que tenga tantas filas como aeropuertos distintos de *origen* existan. ¿Cuántas filas tiene? Almacenar dicho recuento en la variable entera `n_origen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8fb2ec5d49ff84edae4833eca797068b",
     "grade": false,
     "grade_id": "ejercicio-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "aeropuertosOrigenDF = flightsConvertido.select(\"origin\").distinct()\n",
    "n_origen = aeropuertosOrigenDF.count()\n",
    "print(n_origen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Crear un nuevo DataFrame llamado `rutasDistintasDF` que tenga dos columnas `origin`, `dest` y que tenga tantas filas como rutas diferentes existan (es decir, como combinaciones distintas haya entre un origen y un destino). Una vez creado, contar cuántas combinaciones hay, almacenando dicho recuento en la variable entera `n_rutas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    }
   ],
   "source": [
    "rutasDistintasDF = flightsConvertido.select(\"origin\", \"dest\").distinct()\n",
    "n_rutas = rutasDistintasDF.count()\n",
    "print(n_rutas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calcular, *sólo para los vuelos que llegan con* ***retraso positivo***, el retraso medio a la llegada de dichos vuelos, para cada aeropuerto de destino. La nueva columna con el retraso medio a la llegada debe llamarse `retraso_medio`. El DF resultante debe estar ordenado de mayor a menor retraso medio. El código que calcule esto debería ir encapsulado en una función de Python llamada `retrasoMedio` que reciba como argumento un DataFrame y devuelva como resultado el DataFrame con el cálculo descrito anteriormente.\n",
    "  \n",
    "  Una vez hecha la función, invocarla pasándole como argumento `flightsConvertido` y almacenar el resultado devuelto en la variable `retrasoMedioDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "097436083d0007a7d99d5108e1a504c9",
     "grade": false,
     "grade_id": "ejercicio-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "def retrasoMedio(df):                          # Función que recibe un DataFrame y devuelve un nuevo DataFrame con el retraso medio por destino\n",
    "    df_filtrado = df.filter(df.arr_delay > 0)\n",
    "    df_resultado = df_filtrado.groupBy(\"dest\") \\\n",
    "                              .agg(avg(\"arr_delay\").alias(\"retraso_medio\")) \\\n",
    "                              .orderBy(\"retraso_medio\", ascending=False)\n",
    "    return df_resultado\n",
    "\n",
    "retrasoMedioDF = retrasoMedio(flightsConvertido) # DataFrame con el retraso medio por destino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora llamo a la función `retrasoMedio` pasándole como argumento `flightsConvertido`. \n",
    "- ¿Cuáles son los tres aeropuertos con mayor retraso medio? \n",
    "- ¿Cuáles son sus retrasos medios en minutos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|dest|     retraso_medio|\n",
      "+----+------------------+\n",
      "| BOI|             64.75|\n",
      "| HDN|              46.8|\n",
      "| SFO|41.193768844221104|\n",
      "+----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrasoMedioDF = retrasoMedio(flightsConvertido)\n",
    "retrasoMedioDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa9ed55cd86eb0958e150d6a918db1af",
     "grade": false,
     "grade_id": "cell-e577747d4427e32b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aplicación de Spark MLlib\n",
    "\n",
    "A continuación, voy a ajustar un modelo de DecisionTree de Spark para predecir si un vuelo vendrá o no con retraso (problema de clasificación binaria), utilizando como variables predictoras el mes, el día del mes, la hora de partida (`dep_time`), la hora de llegada (`arr_time`), el tipo de avión (`carrier`), la distancia y el tiempo que permanece en el aire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4405b7db7180445df5cacc66d82db53",
     "grade": false,
     "grade_id": "cell-e577747d4427e32a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "En estos datos hay variables numéricas y variables categóricas que ahora mismo están tipadas como numéricas, como por ejemplo el mes del año (`month`), que es en realidad categórica. Hay que indicar a Spark cuáles son categóricas e indexarlas. Para ello tengo que: \n",
    "\n",
    "- Crear un `StringIndexer` al que llamaré `indexerMonth` y otro al que llamaré `indexerCarrier` sobre las variables categóricas `month` y `carrier`. El nombre de las columnas indexadas que se van a crear son, respectivamente, `monthIndexed` y `carrierIndexed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb039584cd14e6bc3434e5be930341e6",
     "grade": false,
     "grade_id": "string-indexer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer # Importo la clase StringIndexer\n",
    "\n",
    "indexerMonth = StringIndexer(inputCol=\"month\", outputCol=\"monthIndexed\") # Creo un objeto StringIndexer para la columna month\n",
    "\n",
    "indexerCarrier = StringIndexer(inputCol=\"carrier\", outputCol=\"carrierIndexed\") # Creo un objeto StringIndexer para la columna carrier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69e024b0baeeb36bb74d136c7e113372",
     "grade": false,
     "grade_id": "cell-e577747d4427e323",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Spark requiere que todas las variables estén en una única columna de tipo vector, por lo que después de indexar estas dos variables, hay que fusionarlas en una columna de tipo vector todas ellas, utilizando un `VectorAssembler`. Por tanto:\n",
    "\n",
    "- Crearé en una variable llamada `vectorAssembler` un `VectorAssembler` que reciba como entrada una lista de todas las variables de entrada (sin incluir `arr_delay`) que serán las que formarán parte del modelo. \n",
    "  \n",
    "    Crearé primero esta lista de variables (lista de strings) en la variable `columnas_ensamblar` y pasaré dicha variable como argumento al crear el `VectorAssembler`. En el caso de las columnas `month` y `carrier`, no usaré las variables originales sino las indexadas en el apartado anterior. La columna de tipo vector creada con las características ensambladas se llamará `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44623471d64d0605fcbcd4bbcc3c2a0d",
     "grade": false,
     "grade_id": "vector-assembler",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler # Importo la clase VectorAssembler\n",
    "\n",
    "columnas_ensamblar = [\"monthIndexed\", \"carrierIndexed\", \"day\", \"dep_time\", \"arr_time\", \"distance\", \"air_time\"] # Columnas que quiero ensamblar\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=columnas_ensamblar, outputCol=\"features\")  # Creo un objeto VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d17b2fa1d8a4bd02b89952429ba1552",
     "grade": true,
     "grade_id": "vector-assembler-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(isinstance(vectorAssembler, VectorAssembler))\n",
    "assert(vectorAssembler.getOutputCol() == \"features\")\n",
    "input_cols = vectorAssembler.getInputCols() \n",
    "assert(len(input_cols) == 7)\n",
    "assert(\"arr_delay\" not in input_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarización de la columna objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8eeb3a3f0b15e8b374789706cd9bce49",
     "grade": false,
     "grade_id": "cell-e577747d4427e32dsdf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finalmente, veo que la columna `arr_delay` es continua, y no binaria como requiere un problema de clasificación con dos clases. Por tanto, voy a convertirla en binaria. Para ello:\n",
    "\n",
    "-  Utilizaré un binarizador de Spark, fijando a 15 el umbral, y guardarlo en la variable `delayBinarizer`. Consideraré retrasado un vuelo que llegue con más de 15 minutos de retraso, y no retrasado en caso contrario. La nueva columna creada con la variable binaria se llamará `arr_delay_binary` y será interpretada como la columna target para el algoritmo. Por ese motivo, esta columna **no** se incluyó en el apartado anterior entre las columnas que se ensamblan para formar las features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f33c493a3c3dd83fb6a39a7acefca5c",
     "grade": false,
     "grade_id": "binarizer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "delayBinarizer = Binarizer(threshold=15, inputCol=\"arr_delay\", outputCol=\"arr_delay_binary\") # Creo un objeto Binarizer para la columna arr_delay con un threshold de 15 minutos de retraso en la llegada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación del modelo de árbol de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5aed16d7ed0219fe1d8741848f594319",
     "grade": false,
     "grade_id": "cell-25a7793978ee7d05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Por último, creo el modelo de clasificación.\n",
    "\n",
    "- Creo en una variable `decisionTree` un árbol de clasificación de Spark (`DecisionTreeClassifier` del paquete `pyspark.ml.classification`)\n",
    "- Indico como columna de entrada la columna creada por el `VectorAssembler`.\n",
    "- Indico como columna objetivo (target) la columna creada por el `Binarizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e785136d6d06691c003ff9542027e03d",
     "grade": false,
     "grade_id": "decision-tree",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "decisionTree = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"arr_delay_binary\") # Creo un objeto DecisionTreeClassifier con las columnas features y arr_delay_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "728da91edabbac5b62baf31bdd0a707e",
     "grade": false,
     "grade_id": "cell-e577747d4427e32d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora encapsulo todas las fases en un sólo pipeline y lo entrenaré:\n",
    "\n",
    "- Crearé en una variable llamada `pipeline` un objeto `Pipeline` de Spark con las etapas anteriores en el orden adecuado para poder entrenar un modelo. \n",
    "\n",
    "- Entrenarlo llamando al método `fit` y guardaré el pipeline entrenado devuelto en una variable llamada `pipelineModel`. \n",
    "\n",
    "- Aplico el pipeline entrenado para predecir el DataFrame `flightsConvertido`, guardando las predicciones devueltas en la variable `flightsPredictions` que será un DataFrame. \n",
    "\n",
    "- Crearé un evaluador con el fin de calcular la precisión del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edbdb627305d03efa41a88426330e160",
     "grade": false,
     "grade_id": "pipeline",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------------+\n",
      "|            features|prediction|arr_delay_binary|\n",
      "+--------------------+----------+----------------+\n",
      "|[10.0,0.0,1.0,1.0...|       1.0|             1.0|\n",
      "|[10.0,6.0,1.0,4.0...|       0.0|             0.0|\n",
      "|[10.0,4.0,1.0,8.0...|       0.0|             0.0|\n",
      "|[10.0,6.0,1.0,28....|       0.0|             0.0|\n",
      "|[10.0,0.0,1.0,34....|       1.0|             1.0|\n",
      "|[10.0,3.0,1.0,37....|       0.0|             1.0|\n",
      "|[10.0,4.0,1.0,346...|       0.0|             1.0|\n",
      "|[10.0,4.0,1.0,526...|       0.0|             0.0|\n",
      "|[10.0,4.0,1.0,527...|       0.0|             1.0|\n",
      "|[10.0,4.0,1.0,536...|       0.0|             0.0|\n",
      "|[10.0,4.0,1.0,541...|       0.0|             0.0|\n",
      "|[10.0,6.0,1.0,549...|       0.0|             0.0|\n",
      "|[10.0,3.0,1.0,550...|       0.0|             0.0|\n",
      "|[10.0,5.0,1.0,557...|       0.0|             0.0|\n",
      "|[10.0,0.0,1.0,557...|       0.0|             0.0|\n",
      "|[10.0,0.0,1.0,558...|       0.0|             0.0|\n",
      "|[10.0,9.0,1.0,559...|       0.0|             0.0|\n",
      "|[10.0,5.0,1.0,600...|       0.0|             0.0|\n",
      "|[10.0,0.0,1.0,600...|       0.0|             0.0|\n",
      "|[10.0,9.0,1.0,602...|       0.0|             0.0|\n",
      "+--------------------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "trainData, testData = flightsConvertido.randomSplit([0.8, 0.2], seed=7) # Divido los datos en entrenamiento y test (80% - 20%)\n",
    "\n",
    "pipeline = Pipeline(stages=[indexerMonth, indexerCarrier, vectorAssembler, delayBinarizer, decisionTree]) # Creo un objeto Pipeline con los distintos objetos\n",
    "\n",
    "pipelineModel = pipeline.fit(flightsConvertido) # Entreno el modelo\n",
    "\n",
    "flightsPredictions = pipelineModel.transform(flightsConvertido) # Realizo las predicciones\n",
    "\n",
    "flightsPredictions.select(\"features\", \"prediction\", \"arr_delay_binary\").show() # Muestro algunas predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c531953abf18cfb3b67571ddde7a57d",
     "grade": false,
     "grade_id": "cell-61156fe5938763f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Muestro la matriz de confusión. Agrupo por la variable que tiene la clase verdadera y la que tiene la clase predicha, para ver en cuántos casos coinciden y en cuántos difieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75f98ae39b827e75c3f0b4b2aaa6b0db",
     "grade": false,
     "grade_id": "cell-896752beb71cb455",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+------+\n",
      "|arr_delay_binary|prediction| count|\n",
      "+----------------+----------+------+\n",
      "|             1.0|       1.0|   530|\n",
      "|             0.0|       1.0|    70|\n",
      "|             1.0|       0.0| 23719|\n",
      "|             0.0|       0.0|136429|\n",
      "+----------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flightsPredictions.groupBy(\"arr_delay_binary\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del modelo: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator( # Creo un evaluador para la precisión del modelo\n",
    "    labelCol=\"arr_delay_binary\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(flightsPredictions) # Calculo la precisión del modelo\n",
    "print(f\"Accuracy del modelo: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manejo de Spark Streaming y Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se va a implementar un flujo de datos en tiempo real utilizando Apache Kafka y Spark Streaming. El objetivo es leer mensajes JSON desde un topic de Kafka, estructurar y transformar estos datos en columnas estructuradas para analizar y actualizar dinámicamente los resultados mediante el cálculo del retraso promedio en los vuelos.\n",
    "\n",
    "Lo primero que haré es crear un topic llamado \"retrasos\" en Apache Kafka. Un topic representa una cola lógica donde se publican y almacenan mensajes. Es el mecanismo principal para organizar y categorizar los datos en Kafka.\n",
    "\n",
    "- Comando de creación del topic -> /usr/lib/kafka/bin/kafka-topics.sh --bootstrap-server cluster1-w-0:9092 --create --replication-factor 1 --partitions 1 --topic retrasos\n",
    "\n",
    "Para comprobar si se ha creado correctamente puedo usar el siguiente comando:\n",
    "- Comando para obtener una lista de los topics existentes -> /usr/lib/kafka/bin/kafka-topics.sh --bootstrap-server cluster1-w-0:9092 --list \n",
    "\n",
    "Debería aparecer que se ha creado el topic \"retrasos\" \n",
    "\n",
    "*Para los comandos anteriores \"cluster1\" debe ser sustituido por el nombre del cluster en cuestión.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partiré de la función `retrasoMedio`, creada en la Tarea 3.\n",
    "\n",
    "**El objetivo es calcular para cada aeropuerto de llegada el retraso medio de los vuelos que llegan con retraso positivo.**\n",
    "\n",
    "Copio la función retrasoMedio, pero le añodo una columna al final llamada `Ingeniero_responsable`. De forma que ahora el DataFrame devuelto por la función tendrá tres columnas: `dest`, `retraso_medio` e `Ingeniero_responsable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def retrasoMedio(df):\n",
    "    df_filtrado = df.filter(df.arr_delay > 0)\n",
    "    df_resultado = df_filtrado.groupBy(\"dest\") \\\n",
    "                              .agg(avg(\"arr_delay\").alias(\"retraso_medio\")) \\\n",
    "                              .orderBy(\"retraso_medio\", ascending=False) \\\n",
    "                              .withColumn(\"Ingeniero_responsable\", F.lit(\"Rubén Jaime\")) # Añado una columna con mi nombre\n",
    "    return df_resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento en tiempo real con Kafka: Transformación y análisis de datos en streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos Kafka para actualizar en tiempo real el resultado calculado en el apartado anterior. \n",
    "\n",
    "Asumiré que los mensajes leídos de Kafka tienen solamente dos campos que son los únicos necesarios para llevar a cabo la operación anterior: dest y arr_delay. La idea será crear un Streaming DataFrame para leer de Kafka, y después a la función retrasoMedio pasándolo como argumento. Voy a leer del topic `retrasos` por lo que debe quedar indicado.\n",
    "\n",
    "El paso a paso para crear la variable `retrasosStreamingDF`, un Streaming DataFrame leyendo de Apache Kafka, es el siguiente:\n",
    "  - Uso la variable `readStream` interna de la SparkSession `spark` (en lugar de `read` como se usó anteriormente).\n",
    "  \n",
    "  - Indico que el formato es `\"kafka\"`.\n",
    "\n",
    "  - Indico cuáles son los brokers de Kafka de los que voy a leer y el puerto al que me quiero conectar para leer (9092 es el que usa Kafka por defecto), con `.option(\"kafka.bootstrap.servers\", \"nombre_cluster-w-0:9092,nombre_cluster-w-1:9092\")`. De esta manera puedo leer el mensaje si el productor de Kafka lo envía a cualquiera de los dos brokers existentes, que son los nodos del cluster identificados como `nombre_cluster-w-0` y `nombre_cluster-w-1`\n",
    "  \n",
    "  - Indico que quiero suscribirme al topic `\"retrasos\"`, así el programa escuchará continuamente el topic retrasos y leerá todos los mensajes que se publiquen en él.\n",
    "  \n",
    "  - Finalmente añado `load()` para realizar la lectura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creación de un DataFrame con el retraso medio por destino y el ingeniero responsable\n",
    "retrasosStreamingDF = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"cluster1-w-0:9092,cluster1-w-1:9092\") \\\n",
    "    .option(\"subscribe\", \"retrasos\") \\\n",
    "    .load()\n",
    "\n",
    "retrasosStreamingDF.printSchema() # Muestro el esquema del DataFrame de streaming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También he mostrado el esquema del DataFrame resultante. Todas las columnas que aparecen son creadas automáticamente por Spark cuando leemos de Kafka. De ellas, la que me interesa es `value` que contiene el mensaje de Kafka, en formato de datos binarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendré que estructurar estos datos para poder extraer los campos. Para ello:\n",
    "\n",
    "- Convierto la columna `value` a tipo String. De esta forma tendremos una columna que contendrá en cada fila** un fichero JSON completo. \n",
    "\n",
    "- Necesito extraer los dos campos de cada uno de los JSON y convertirlos en una columna llamada `parejas`, de tipo `struct`, utilizó la función `from_json` de Spark, que se aplica a cada fila de la columna \"value\" y parsea el String según un esquema indicado, devolviendo una columna de tipo `struct` (una estructura formada por dos campos de tipo String e Integer respectivamente).\n",
    "\n",
    "- `parejas` al ser de tipo `struct`, permite acceder a cada uno de sus dos campos (`dest` y `arr_delay`) con el operador . (punto). Utilizando `withColumn` dos veces, crea dos columnas llamadas `dest` y `arr_delay` como el resultado de acceder a `parejas.dest` y `parejas.arr_delay` respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "esquema = StructType([                              # Creo un esquema para el DataFrame de streaming\n",
    "    StructField(\"dest\", StringType(), True),        # Campo \"dest\" de tipo StringType\n",
    "    StructField(\"arr_delay\", DoubleType(), True)    # Campo \"arr_delay\" de tipo DoubleType\n",
    "])\n",
    "\n",
    "# Parseo el DataFrame de streaming para obtener las columnas \"dest\" y \"arr_delay\" con los tipos de datos definidos en el esquema\n",
    "parsedDF = retrasosStreamingDF \\\n",
    "    .select(\"value\") \\\n",
    "    .withColumn(\"value\", F.col(\"value\").cast(StringType())) \\\n",
    "    .withColumn(\"parejas\", F.from_json(F.col(\"value\"), esquema)) \\\n",
    "    .withColumn(\"dest\", F.col(\"parejas.dest\")) \\\n",
    "    .withColumn(\"arr_delay\", F.col(\"parejas.arr_delay\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El DataFrame ya contiene una columna `dest` con el nombre del aeropuerto destino y una columna de números reales `arr_delay` con el retraso. Ahora puedo efectuar el mismo tipo de agregación que estaba haciendo en la función `retrasoMedio`. Por tanto, llamo a `retrasoMedio` pasando `parsedDF` como argumento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/22 11:59:33 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-409f0c09-93e5-4fc0-9a40-8b50dad64562. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/01/22 11:59:33 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "retrasoMedioStreamingDF = retrasoMedio(parsedDF) # Creo un DataFrame de streaming con el retraso medio por destino y el ingeniero responsable\n",
    "\n",
    "# Escribo el DataFrame de streaming en memoria para poder consultarlo con Spark SQL cuando se va actualizando y arranco la ejecución en streaming con start() \n",
    "consoleOutput = retrasoMedioStreamingDF\\\n",
    "                    .writeStream\\\n",
    "                    .queryName(\"retrasosAgg\")\\\n",
    "                    .outputMode(\"complete\")\\\n",
    "                    .format(\"memory\")\\\n",
    "                    .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la celda anterior ejecutada, abro el kafka-console-producer (que es un productor de Kafka que envía los mensajes que escribamos por teclado al topic que le indiquemos) entrando por SSH a cualquiera de las máquinas. Hay que cambiar el nombre del clúster por el que se tenga en cada caso (se puede ver en la propia terminal, justo detrás de @: es el nombre que viene antes de «-m»): \n",
    "\n",
    "Comando para ejecutar el kafka-console-producer -> /usr/lib/kafka/bin/kafka-console-producer.sh --broker-list cluster1-w-0:9092 --topic retrasos \n",
    "\n",
    "- El productor de consola suele utilizarse para desarrollo y testeo, pero nunca para entornos productivos. Tras cada mensaje, debemos pulsar ENTER. El nombre del bróker de Kafka al que va dirigido el mensaje puede ser tanto el worker 0 como el 1 del clúster. Lo que se indica es el nombre de la máquina, como nombrecluster-w-0 (o bien w-1).  \n",
    "\n",
    "- Cada mensaje debe ser una línea de texto con estructura JSON. Tras cada mensaje, debemos pulsar ENTER para que el productor lo envíe a Kafka.  \n",
    "\n",
    "- El programa no admite borrado de caracteres, pero puede finalizarse en cualquier momento pulsando Ctrl + C, y volverse a ejecutar escribiendo de nuevo el comando anterior en la terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada vez que pegue un mensaje, ejecutaré la consulta `select * from retrasosAgg` a través del método `spark.sql(...)` y mostraré el DataFrame `agregadosDF` devuelto por dicho método. Eso hará una consulta contra la vista temporal (volátil) `retrasosAgg` que se ha creado en el metastore de Hive gracias al `writeStream` anterior. \n",
    "\n",
    "Ejecutaré la celda de `show` tantas veces como sea necesario hasta ver un resultado distinto al que se ve en la ejecución anterior, para asegurarme de que Spark ya ha leído e incorporado el nuevo dato en su cálculo de la agregación y por tanto ha actualizado el resultado.\n",
    "\n",
    "El método `.sql(...)` es una transformación, y por tanto, se re-ejecuta la consulta cada vez que invoco a la acción `show()` sobre el resultado, ya que no voy a cachear nada, precisamente para forzar la reevaluación de la consulta y poder ver así el contenido actualizado de dicha tabla (en memoria) de Hive cada vez que hacemos `show()`.\n",
    "\n",
    "- No hay problema por evaluar muchas veces una misma celda, ya que el cálculo sólo se actualizará una vez. Las siguientes veces que la ejecute seguirá mostrando el mismo resultado mientras no se envíe otro nuevo mensaje en Kafka.\n",
    "\n",
    "Introduciré los siguientes mensajes JSON uno por uno en el productor de consola:\n",
    "\n",
    "- {\"dest\": \"GRX\", \"arr_delay\": 2.6}\n",
    "- {\"dest\": \"MAD\", \"arr_delay\": 5.4}\n",
    "- {\"dest\": \"GRX\", \"arr_delay\": 1.5}\n",
    "- {\"dest\": \"MAD\", \"arr_delay\": 20.0}\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/10kafkamensajes.png\">\n",
    "</div>\n",
    "\n",
    "\n",
    "Estos mensajes tienen un campo `dest` y un campo `arr_delay`, simulando la información que estaríamos recibiendo en tiempo real de los distintos aeropuertos a medida que los vuelos van aterrizando. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:=====================================================>(199 + 1) / 200]\r"
     ]
    }
   ],
   "source": [
    "agregadosDF = spark.sql(\"SELECT * FROM retrasosAgg\") # Consulto el DataFrame de streaming con Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+---------------------+\n",
      "|dest|retraso_medio|Ingeniero_responsable|\n",
      "+----+-------------+---------------------+\n",
      "| GRX|          2.6|          Rubén Jaime|\n",
      "+----+-------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "agregadosDF.show() # Ejecuto varias veces esta celda tras enviar el primer mensaje, hasta ver que el DataFrame no es vacío"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se recibe correctamente el primer mensaje. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+---------------------+\n",
      "|dest|retraso_medio|Ingeniero_responsable|\n",
      "+----+-------------+---------------------+\n",
      "| MAD|          5.4|          Rubén Jaime|\n",
      "| GRX|          2.6|          Rubén Jaime|\n",
      "+----+-------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "agregadosDF.show() # Ejecuto varias veces esta celda tras enviar el segundo mensaje, hasta ver que el DataFrame ha cambiado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se recibe correctamente el segundo mensaje. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+---------------------+\n",
      "|dest|retraso_medio|Ingeniero_responsable|\n",
      "+----+-------------+---------------------+\n",
      "| MAD|          5.4|          Rubén Jaime|\n",
      "| GRX|         2.05|          Rubén Jaime|\n",
      "+----+-------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "agregadosDF.show() # Ejecuta varias veces esta celda tras enviar el tercer mensaje, hasta ver que el DataFrame ha cambiado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se recibe correctamente el tercer mensaje, ya que el retraso medio de GRX ahora refleja la media del retraso del 1er mensaje y del 3er mensaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+---------------------+\n",
      "|dest|retraso_medio|Ingeniero_responsable|\n",
      "+----+-------------+---------------------+\n",
      "| MAD|         12.7|          Rubén Jaime|\n",
      "| GRX|         2.05|          Rubén Jaime|\n",
      "+----+-------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agregadosDF.show() # Ejecuta varias veces esta celda tras enviar el cuarto mensaje, hasta ver que el DataFrame ha cambiado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se recibe correctamente el cuarto mensaje, ya que el retraso medio de MAD ahora refleja la media del retraso del 2o mensaje y del 4o mensaje, consiguiendo así el objetivo propuesto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
